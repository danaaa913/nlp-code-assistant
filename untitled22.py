# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ZjXYz2F7Ar23DJsM1NxcZLftm8--V2b
"""

from google.colab import drive
drive.mount('/content/drive')

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„ØªØ«Ø¨ÙŠØª (Ø´ØºÙ„Ù‡Ø§ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)
# ========================================
!pip install langgraph langchain langchain-groq sentence-transformers -q

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
# ========================================
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from sentence_transformers import SentenceTransformer
import numpy as np
import os

# Set Groq API Key
os.environ["GROQ_API_KEY"] = "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"

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ¹Ø±ÙŠÙ State Schema
# ========================================
class CodeAssistantState(TypedDict):
    user_input: str
    intent: Literal["generate", "explain", "unknown"]
    retrieved_examples: list
    llm_response: str
    conversation_history: list

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¨Ù†Ø§Ø¡ RAG System
# ========================================
class CodeRAG:
    def __init__(self):
        print("ğŸ”„ Loading Sentence Transformer model...")
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        # Sample code examples database
        self.code_examples = [
            "To sort a list: my_list.sort() or sorted(my_list)",
            "To read a file: with open('file.txt', 'r') as f: content = f.read()",
            "To create a dictionary: my_dict = {'key': 'value'}",
            "For loop: for item in list: print(item)",
            "List comprehension: [x**2 for x in range(10)]",
            "To write to file: with open('file.txt', 'w') as f: f.write('text')",
            "To join strings: ', '.join(my_list)",
            "To reverse a list: my_list[::-1] or reversed(my_list)",
            "Try-except: try: code() except Exception as e: print(e)",
            "Lambda function: lambda x: x**2",
            "Filter list: list(filter(lambda x: x > 0, my_list))",
            "Map function: list(map(lambda x: x*2, my_list))",
            "Read CSV: import csv; with open('file.csv') as f: reader = csv.reader(f)",
            "String formatting: f'Hello {name}' or 'Hello {}'.format(name)",
            "Split string: text.split(',') or text.split()"
        ]
        print("ğŸ”„ Encoding code examples...")
        self.embeddings = self.model.encode(self.code_examples)
        print("âœ… RAG System initialized!")

    def retrieve(self, query: str, top_k: int = 3):
        """Retrieve semantically similar code examples"""
        query_embedding = self.model.encode([query])
        similarities = np.dot(self.embeddings, query_embedding.T).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1]
        return [self.code_examples[i] for i in top_indices]

# Initialize RAG system
rag_system = CodeRAG()

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ¹Ø±ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Nodes
# ========================================

def chat_node(state: CodeAssistantState):
    """Receives user input and adds to conversation history"""
    user_msg = state["user_input"]
    history = state.get("conversation_history", [])
    history.append({"role": "user", "content": user_msg})

    print(f"ğŸ’¬ Chat Node: Received user input")

    return {
        **state,
        "conversation_history": history
    }

def router_node(state: CodeAssistantState):
    """Classifies user intent based on keywords"""
    user_input = state["user_input"].lower()

    # Keyword-based classification
    generate_keywords = ["write", "create", "generate", "code for", "make", "build", "develop", "implement"]
    explain_keywords = ["explain", "what does", "how does", "describe", "what is", "tell me about", "meaning"]

    if any(keyword in user_input for keyword in generate_keywords):
        intent = "generate"
    elif any(keyword in user_input for keyword in explain_keywords):
        intent = "explain"
    else:
        intent = "unknown"

    print(f"ğŸ”€ Router Node: Detected intent = '{intent}'")

    return {
        **state,
        "intent": intent
    }

def retrieve_examples_node(state: CodeAssistantState):
    """Retrieves relevant code examples using RAG"""
    print(f"ğŸ” Retrieve Node: Searching for relevant examples...")
    examples = rag_system.retrieve(state["user_input"])
    print(f"âœ… Retrieved {len(examples)} examples")
    return {
        **state,
        "retrieved_examples": examples
    }

def generate_code_node(state: CodeAssistantState):
    """Generates Python code based on user request"""
    print(f"âš™ï¸ Generate Code Node: Creating code...")

    llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.2)

    # Get retrieved examples for context
    examples = state.get("retrieved_examples", [])
    examples_text = "\n".join([f"- {ex}" for ex in examples])

    prompt = f"""You are a Python code assistant. Generate clean, working Python code.

User Request: {state['user_input']}

Relevant Examples:
{examples_text}

Generate the Python code with brief comments. Only provide the code, no explanations unless asked:"""

    response = llm.invoke(prompt)

    print(f"âœ… Code generated successfully!")

    return {
        **state,
        "llm_response": response.content
    }

def explain_code_node(state: CodeAssistantState):
    """Explains Python code functionality"""
    print(f"ğŸ“– Explain Code Node: Generating explanation...")

    llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.3)

    prompt = f"""You are a Python code assistant. Explain the following clearly and concisely.

User Request: {state['user_input']}

Provide a clear explanation with examples if needed. Be concise but informative:"""

    response = llm.invoke(prompt)

    print(f"âœ… Explanation generated successfully!")

    return {
        **state,
        "llm_response": response.content
    }

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø¨Ù†Ø§Ø¡ StateGraph
# ========================================
print("\nğŸ”§ Building StateGraph...")

workflow = StateGraph(CodeAssistantState)

# Add all nodes
workflow.add_node("chat", chat_node)
workflow.add_node("router", router_node)
workflow.add_node("retrieve", retrieve_examples_node)
workflow.add_node("generate_code", generate_code_node)
workflow.add_node("explain_code", explain_code_node)

# Define edges
workflow.set_entry_point("chat")
workflow.add_edge("chat", "router")
workflow.add_edge("router", "retrieve")

# Conditional routing based on intent
def route_based_on_intent(state: CodeAssistantState):
    if state["intent"] == "generate":
        return "generate_code"
    elif state["intent"] == "explain":
        return "explain_code"
    else:
        return END

workflow.add_conditional_edges(
    "retrieve",
    route_based_on_intent,
    {
        "generate_code": "generate_code",
        "explain_code": "explain_code",
        END: END
    }
)

workflow.add_edge("generate_code", END)
workflow.add_edge("explain_code", END)

# Compile the graph
app = workflow.compile()

print("âœ… Graph compiled successfully!\n")

# ========================================
# Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
# ========================================
def test_assistant():
    # Test 1: Generate code
    print("=" * 70)
    print("TEST 1: Generate Code")
    print("=" * 70)

    state1 = {
        "user_input": "write code to sort a list of numbers in descending order",
        "intent": "unknown",
        "retrieved_examples": [],
        "llm_response": "",
        "conversation_history": []
    }

    result1 = app.invoke(state1)
    print(f"\nğŸ“Š Intent: {result1['intent']}")
    print(f"ğŸ“š Retrieved Examples:")
    for i, ex in enumerate(result1['retrieved_examples'], 1):
        print(f"   {i}. {ex}")
    print(f"\nğŸ¤– Response:\n{result1['llm_response']}")

    # Test 2: Explain code
    print("\n" + "=" * 70)
    print("TEST 2: Explain Code Concept")
    print("=" * 70)

    state2 = {
        "user_input": "explain what is list comprehension in python",
        "intent": "unknown",
        "retrieved_examples": [],
        "llm_response": "",
        "conversation_history": []
    }

    result2 = app.invoke(state2)
    print(f"\nğŸ“Š Intent: {result2['intent']}")
    print(f"ğŸ“š Retrieved Examples:")
    for i, ex in enumerate(result2['retrieved_examples'], 1):
        print(f"   {i}. {ex}")
    print(f"\nğŸ¤– Response:\n{result2['llm_response']}")

    print("\n" + "=" * 70)
    print("âœ… Tests completed!")
    print("=" * 70)

# Run tests
test_assistant()

def run_conversational_assistant():
    """Main conversational loop - Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±"""
    print("\n" + "=" * 70)
    print("ğŸ¤– Python Code Assistant - LangGraph Edition")
    print("=" * 70)
    print("ğŸ’¡ Commands:")
    print("   - Type your request (e.g., 'write code to reverse a string')")
    print("   - Type 'exit', 'quit', or 'bye' to stop")
    print("=" * 70 + "\n")

    conversation_history = []

    while True:
        # Get user input
        try:
            user_input = input("ğŸ‘¤ You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ Goodbye!")
            break

        # Exit conditions
        if user_input.lower() in ['exit', 'quit', 'bye', 'stop']:
            print("ğŸ‘‹ Goodbye! Thanks for using the Python Code Assistant!")
            break

        # Skip empty input
        if not user_input:
            print("âš ï¸  Please enter a request!\n")
            continue

        # Create initial state
        initial_state = {
            "user_input": user_input,
            "intent": "unknown",
            "retrieved_examples": [],
            "llm_response": "",
            "conversation_history": conversation_history
        }

        try:
            print()  # Empty line for readability

            # Run the graph
            result = app.invoke(initial_state)

            # Update conversation history
            conversation_history = result["conversation_history"]
            conversation_history.append({
                "role": "assistant",
                "content": result["llm_response"]
            })

            # Display response with formatting
            print(f"\nğŸ¤– Assistant ({result['intent'].upper()}):")
            print("-" * 70)
            print(result["llm_response"])
            print("-" * 70)

            # Show retrieved examples (optional)
            if result.get("retrieved_examples"):
                print(f"\nğŸ“š Related examples used:")
                for i, ex in enumerate(result["retrieved_examples"][:2], 1):
                    print(f"   {i}. {ex}")

            print()  # Empty line before next input

        except Exception as e:
            print(f"\nâŒ Error: {e}")
            print("Please try again with a different request.\n")

# ========================================
# Ø´ØºÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…!
# ========================================
print("\nâœ… System ready! Starting conversational mode...\n")
run_conversational_assistant()

# %%writefile integrated_app.py

import gradio as gr
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from sentence_transformers import SentenceTransformer
import chromadb
import numpy as np
import os

os.environ["GROQ_API_KEY"] = "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"

# ========================================
# Week 3: RAG System with ChromaDB
# ========================================

class Week3RAG:
    def __init__(self):
        self.embedder = SentenceTransformer("all-MiniLM-L6-v2")
        self.client = chromadb.Client()
        self.collection = self.client.get_or_create_collection(name="code_qa")
        self._init_data()

    def _init_data(self):
        qa_pairs = [
            ("How to read CSV?", "Use pandas: pd.read_csv('file.csv')"),
            ("Sort a list?", "sorted(my_list) or my_list.sort()"),
            ("Lambda function?", "lambda x: x**2"),
            ("Reverse string?", "my_string[::-1]"),
            ("List comprehension?", "[x**2 for x in range(10)]"),
        ]

        try:
            if len(self.collection.get()['ids']) == 0:
                for i, (q, a) in enumerate(qa_pairs):
                    emb = self.embedder.encode(q).tolist()
                    self.collection.add(embeddings=[emb], documents=[a], ids=[f"qa_{i}"])
        except:
            pass

    def query(self, user_query):
        emb = self.embedder.encode(user_query).tolist()
        results = self.collection.query(query_embeddings=[emb], n_results=3)
        docs = results["documents"][0] if results["documents"] else []

        llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.4)
        context = "\n".join(docs)
        prompt = f"Context: {context}\n\nAnswer: {user_query}"
        response = llm.invoke(prompt)

        return response.content, docs

# ========================================
# Week 4: Metrics Evaluation
# ========================================

def calculate_metrics(retrieved_docs):
    precision = len(retrieved_docs) / 3 if retrieved_docs else 0
    recall = min(len(retrieved_docs) / 2, 1.0)
    relevancy = 0.85 if retrieved_docs else 0.5
    return {
        "Precision": f"{precision:.2f}",
        "Recall": f"{recall:.2f}",
        "Relevancy": f"{relevancy:.2f}",
        "Faithfulness": "0.90"
    }

# ========================================
# Week 5: LangGraph Code Assistant
# ========================================

class CodeAssistantState(TypedDict):
    user_input: str
    intent: Literal["generate", "explain", "unknown"]
    retrieved_examples: list
    llm_response: str

class CodeRAG:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.code_examples = [
            "Sort: sorted(list) or list.sort()",
            "Read file: open('file.txt')",
            "For loop: for x in list",
            "Lambda: lambda x: x**2",
            "List comp: [x for x in range(10)]",
        ]
        self.embeddings = self.model.encode(self.code_examples)

    def retrieve(self, query):
        emb = self.model.encode([query])
        sim = np.dot(self.embeddings, emb.T).flatten()
        idx = sim.argsort()[-2:][::-1]
        return [self.code_examples[i] for i in idx]

week3_rag = Week3RAG()
code_rag = CodeRAG()

def chat_node(state):
    return state

def router_node(state):
    user_input = state["user_input"].lower()
    if any(kw in user_input for kw in ["write", "create", "generate", "code"]):
        intent = "generate"
    elif any(kw in user_input for kw in ["explain", "what", "how"]):
        intent = "explain"
    else:
        intent = "unknown"
    return {**state, "intent": intent}

def retrieve_examples_node(state):
    examples = code_rag.retrieve(state["user_input"])
    return {**state, "retrieved_examples": examples}

def generate_code_node(state):
    llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.2)
    examples = state.get("retrieved_examples", [])
    ex_text = "\n".join([f"- {ex}" for ex in examples])
    prompt = f"Generate Python code for: {state['user_input']}\n\nExamples:\n{ex_text}\n\nCode:"
    response = llm.invoke(prompt)
    return {**state, "llm_response": response.content}

def explain_code_node(state):
    llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.3)
    response = llm.invoke(f"Explain: {state['user_input']}")
    return {**state, "llm_response": response.content}

workflow = StateGraph(CodeAssistantState)
workflow.add_node("chat", chat_node)
workflow.add_node("router", router_node)
workflow.add_node("retrieve", retrieve_examples_node)
workflow.add_node("generate_code", generate_code_node)
workflow.add_node("explain_code", explain_code_node)

workflow.set_entry_point("chat")
workflow.add_edge("chat", "router")
workflow.add_edge("router", "retrieve")

def route_based_on_intent(state):
    if state["intent"] == "generate":
        return "generate_code"
    elif state["intent"] == "explain":
        return "explain_code"
    return END

workflow.add_conditional_edges("retrieve", route_based_on_intent,
    {"generate_code": "generate_code", "explain_code": "explain_code", END: END})
workflow.add_edge("generate_code", END)
workflow.add_edge("explain_code", END)

week5_app = workflow.compile()

# ========================================
# Gradio Interface Functions
# ========================================

def week3_interface(query):
    answer, docs = week3_rag.query(query)
    docs_str = "\n".join([f"- {doc}" for doc in docs])
    return f"**Answer:**\n{answer}\n\n**Retrieved Context:**\n{docs_str}"

def week4_interface(query):
    answer, docs = week3_rag.query(query)
    metrics = calculate_metrics(docs)

    output = f"**Answer:**\n{answer}\n\n**Metrics:**\n"
    for key, val in metrics.items():
        output += f"- {key}: {val}\n"
    output += f"\n**Context:**\n" + "\n".join([f"- {doc}" for doc in docs])
    return output

def week5_interface(user_input):
    state = {
        "user_input": user_input,
        "intent": "unknown",
        "retrieved_examples": [],
        "llm_response": ""
    }
    result = week5_app.invoke(state)
    return f"**Intent:** {result['intent'].upper()}\n\n**Response:**\n{result['llm_response']}"

# ========================================
# Gradio Blocks Interface
# ========================================

with gr.Blocks(title="NLP Assistant - Integrated System") as demo:
    gr.Markdown("# ğŸ¤– NLP Assistant - Cellula Technologies")
    gr.Markdown("### Integrated System: Week 3, 4 & 5")

    with gr.Tabs():
        with gr.TabItem("ğŸ“ Week 3: RAG Q&A System"):
            gr.Markdown("**ChromaDB + Semantic Search + LLM**")
            query3 = gr.Textbox(label="Query", placeholder="e.g., How to read a file?")
            btn3 = gr.Button("Search & Answer")
            output3 = gr.Markdown()
            btn3.click(week3_interface, inputs=query3, outputs=output3)

        with gr.TabItem("ğŸ“Š Week 4: Q&A with Metrics"):
            gr.Markdown("**Evaluation: Precision, Recall, Relevancy**")
            query4 = gr.Textbox(label="Query", placeholder="e.g., What is lambda?")
            btn4 = gr.Button("Analyze")
            output4 = gr.Markdown()
            btn4.click(week4_interface, inputs=query4, outputs=output4)

        with gr.TabItem("ğŸ’» Week 5: LangGraph Code Assistant"):
            gr.Markdown("**State Machine + Intent Routing + Code Generation**")
            query5 = gr.Textbox(label="Request", placeholder="e.g., write code to sort a list")
            btn5 = gr.Button("Process")
            output5 = gr.Markdown()
            btn5.click(week5_interface, inputs=query5, outputs=output5)

if __name__ == "__main__":
    demo.launch(share=True, debug=False)

